{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2674b84c-5201-43fb-a9e0-a7f8c1217b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Bernoulli, Categorical\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9247ade7-a05d-4848-a2df-1cb24ec9a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, input_size, hidden_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Linear(self.input_size, self.hidden_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers.append(nn.Linear(self.hidden_size, 2*self.latent_size))\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        mu, log_sigma2 = torch.split(self.net(X), self.latent_size, dim=-1)\n",
    "        return mu, log_sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc5e4d63-ce14-45d2-bf55-ebc3223aadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, latent_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Linear(self.latent_size, self.hidden_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(self.num_layers - 1):\n",
    "            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers.append(nn.Linear(self.hidden_size, self.output_size))\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.net(X)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0f42fd-7626-4891-a611-7de8392b587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            num_layers=num_layers,\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            latent_size=latent_size\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_layers,\n",
    "            latent_size=latent_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=input_size\n",
    "        )\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass returns everything needed to compute ELBO.\n",
    "        No randomness is created here except through reparameterization input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Encode\n",
    "        mu, log_sigma2 = self.encoder(x)\n",
    "\n",
    "        # Reparameterization (SGVB estimator)\n",
    "        epsilon = torch.randn_like(mu)\n",
    "        sigma = torch.exp(0.5 * log_sigma2)\n",
    "        z = mu + sigma * epsilon\n",
    "\n",
    "        # Decode (logits for Bernoulli likelihood)\n",
    "        logits = self.decoder(z)\n",
    "\n",
    "        return logits, mu, log_sigma2\n",
    "\n",
    "    def _init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba386991-a4ff-4d9d-83e9-b57ee0cc41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_elbo(logits, labels, mu, log_sigma2):\n",
    "    reconstruction = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    reconstruction_term = reconstruction(logits, labels)\n",
    "    sigma2 = torch.exp(log_sigma2)\n",
    "    kl_divergence = 0.5 * torch.sum(torch.square(mu) + sigma2 - log_sigma2 - 1)\n",
    "    return kl_divergence - reconstruction_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a7c724-1263-438e-b525-667e5cb0598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, update_freq=10):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, (features, _) in enumerate(dataloader):\n",
    "        features = features.to(device)\n",
    "        labels = features\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        features = features.view(features.size(0), -1)  # Flatten\n",
    "        labels = labels.view(labels.size(0), -1)        # flatten labels\n",
    "        logits, mu, log_sigma2 = model(features)\n",
    "\n",
    "        loss = criterion(logits, labels, mu, log_sigma2)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if(batch_idx+1)%update_freq == 0:\n",
    "            print(f'   Batch [{batch_idx+1}/{len(dataloader)}] - 'f'Loss: {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = total_loss/num_batches\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d86a444f-f53e-4c3f-a404-8384bc3a0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device, use_mean=False):\n",
    "    \"\"\"\n",
    "    Evaluates average negative ELBO.\n",
    "    \n",
    "    Args:\n",
    "        use_mean:\n",
    "            If True  -> z = mu (deterministic reconstruction)\n",
    "            If False -> z sampled via reparameterization (L=1 Monte Carlo)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    all_reconstructions = []\n",
    "    all_originals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, _ in dataloader:\n",
    "            features = features.to(device)\n",
    "\n",
    "            # Flatten\n",
    "            features = features.view(features.size(0), -1)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, mu, log_sigma2 = model(features)\n",
    "\n",
    "            # Loss = negative ELBO\n",
    "            loss = criterion(logits, features, mu, log_sigma2)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Convert logits â†’ probabilities for visualization\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            all_reconstructions.append(probs.cpu())\n",
    "            all_originals.append(features.cpu())\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    all_reconstructions = torch.cat(all_reconstructions, dim=0)\n",
    "    all_originals = torch.cat(all_originals, dim=0)\n",
    "\n",
    "    return avg_loss, all_reconstructions, all_originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d2929c6-b19a-4bb7-aa7f-6025fdb58907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(num_epochs, model, train_dataloader, test_dataloader, criterion, optimizer, device, update_freq):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        #Train\n",
    "        print(\"INFO - Training...\")\n",
    "        train_loss = train_epoch(model, train_dataloader, criterion, optimizer, device, update_freq=update_freq)\n",
    "        \n",
    "        #Eval\n",
    "        print(\"INFO - Evaluating...\")\n",
    "        test_loss, _, _ = evaluate(model, test_dataloader, criterion, device)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… Training completed!\")\n",
    "    print(f\"ðŸ•’ Total training time: {total_time:.2f}s ({total_time/60:.1f} minutes)\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b3297c7-42ba-427a-b00a-81b44adf834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_MNIST():\n",
    "    # Define data transforms for preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Pad(2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # Mean and Std for MNIST\n",
    "    ])\n",
    "    \n",
    "    # Load training dataset\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='../data/train',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Load test dataset\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='../data/test',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83d434e-15d3-4387-92ad-9ff8471caa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DataLoaders(train_dataset, test_dataset, batch_size, shuffle_train, num_workers):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_train,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScratchVision",
   "language": "python",
   "name": "scratchvision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
